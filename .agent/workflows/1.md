---
description: Initial Setup
---

# Wafaa Workflows 1: Setup & Development

## Project Setup

### 1. Initial Setup
```bash
# Clone & environment
git clone https://github.com/wafaa/ai-concierge.git && cd ai-concierge
python3.11 -m venv venv && source venv/bin/activate
pip install -r backend/requirements.txt

# Start databases
docker run -d --name wafaa-postgres -e POSTGRES_USER=wafaa \
  -e POSTGRES_PASSWORD=dev -e POSTGRES_DB=wafaa -p 5432:5432 postgres:15
docker run -d --name wafaa-redis -p 6379:6379 redis:7-alpine

# Migrations
cd backend && alembic upgrade head
```

### 2. Vector DB Setup
```bash
# Pinecone (cloud)
export PINECONE_API_KEY=xxx PINECONE_ENVIRONMENT=us-east-1
python scripts/setup_vector_db.py --provider pinecone

# OR Weaviate (local)
docker run -d --name wafaa-weaviate -p 8080:8080 \
  -e AUTHENTICATION_ANONYMOUS_ACCESS_ENABLED=true semitechnologies/weaviate:latest
python scripts/setup_vector_db.py --provider weaviate
```

### 3. Environment (.env)
```bash
APP_ENV=development
DATABASE_URL=postgresql+asyncpg://wafaa:dev@localhost/wafaa
REDIS_URL=redis://localhost:6379
VECTOR_DB_PROVIDER=pinecone
PINECONE_API_KEY=xxx
OPENAI_API_KEY=sk-xxx
META_APP_ID=xxx
META_APP_SECRET=xxx
JWT_SECRET_KEY=xxx
```

### 4. Start Server
```bash
uvicorn app.main:app --reload --port 8000
# Separate terminal: celery -A app.worker worker --loglevel=info
```

## Add New Client

### API Call
```bash
curl -X POST http://localhost:8000/api/v1/clients \
  -H "Content-Type: application/json" \
  -H "Authorization: Bearer ADMIN_TOKEN" \
  -d '{
    "name": "Pizza Palace",
    "email": "admin@pizzapalace.com",
    "subscription_tier": "premium"
  }'
```

### Backend Code
```python
# app/services/client_service.py
async def create_client(data: ClientCreate) -> Client:
    client = Client(
        id=uuid.uuid4(),
        name=data.name,
        slug=generate_slug(data.name),
        email=data.email,
        is_active=True
    )
    db.add(client)
    await db.commit()
    # Initialize resources
    await vector_store.create_namespace(str(client.id))
    return client
```

## WhatsApp Integration

### 1. Configure Webhook
```python
# app/api/v1/endpoints/webhooks.py
@router.get("/webhook/whatsapp")
async def verify_webhook(request: Request):
    params = request.query_params
    if params.get("hub.mode") == "subscribe" and \
       params.get("hub.verify_token") == VERIFY_TOKEN:
        return int(params.get("hub.challenge"))
    raise HTTPException(403)

@router.post("/webhook/whatsapp/{client_slug}")
async def whatsapp_webhook(client_slug: str, request: Request):
    # Verify signature
    if not verify_signature(request): raise HTTPException(403)
    payload = await request.json()
    # Process async
    await process_message.delay(client_slug, payload)
    return {"status": "ok"}
```

### 2. Message Processing
```python
# app/services/channels/whatsapp.py
async def process_message(client_slug: str, payload: dict):
    message = payload["entry"][0]["changes"][0]["value"]["messages"][0]
    sender = message["from"]
    text = message["text"]["body"]
    
    client = await get_client_by_slug(client_slug)
    TenantContext.set_tenant(client.id)
    
    response = await orchestrator.process_message(
        message=text, sender_id=sender, channel="whatsapp", client_id=client.id
    )
    
    await send_whatsapp_message(sender, response, client.whatsapp_config)
```

### 3. Send Message
```python
async def send_whatsapp_message(to: str, message: str, config: dict):
    url = f"https://graph.facebook.com/v18.0/{config['phone_id']}/messages"
    headers = {"Authorization": f"Bearer {config['token']}"}
    payload = {
        "messaging_product": "whatsapp",
        "to": to,
        "type": "text",
        "text": {"body": message}
    }
    async with httpx.AsyncClient() as client:
        await client.post(url, json=payload, headers=headers)
```

## Knowledge Base Upload

### 1. Upload API
```bash
curl -X POST http://localhost:8000/api/v1/clients/{id}/knowledge/upload \
  -H "Authorization: Bearer TOKEN" \
  -F "file=@menu.pdf" \
  -F "document_type=menu"
```

### 2. Document Processing
```python
# app/services/knowledge/indexer.py
async def process_document(file_path: str, client_id: UUID, doc_type: str):
    # Parse
    text = await parse_pdf(file_path) if file_path.endswith('.pdf') else read_text(file_path)
    
    # Chunk (1000 chars, 200 overlap)
    chunks = chunk_text(text, size=1000, overlap=200)
    
    # Embed
    embeddings = await embedder.embed_batch([c.text for c in chunks])
    
    # Store in vector DB
    vectors = [{
        "id": f"{client_id}_{doc_type}_{i}",
        "values": emb,
        "metadata": {"client_id": str(client_id), "text": chunk.text, "type": doc_type}
    } for i, (chunk, emb) in enumerate(zip(chunks, embeddings))]
    
    await vector_store.upsert(vectors, namespace=str(client_id))
    
    # Save to DB
    doc = KnowledgeDocument(id=uuid.uuid4(), client_id=client_id, 
                            document_type=doc_type, chunk_count=len(chunks))
    db.add(doc)
    await db.commit()
    return doc
```

## RAG Pipeline

### Complete Flow
```python
# app/services/ai/rag_engine.py
async def generate_response(query: str, client_id: UUID, history: List = None):
    # 1. Retrieve
    query_emb = await embedder.embed(query)
    docs = await vector_store.query(
        vector=query_emb,
        filter={"client_id": str(client_id)},
        namespace=str(client_id),
        top_k=5
    )
    
    # 2. Build prompt
    context = "\n".join([d['metadata']['text'] for d in docs['matches']])
    client = await get_client(client_id)
    settings = await get_settings(client_id)
    
    prompt = f"""You are AI for {client.name}.
RULES: Use only provided context. {settings.response_tone} tone.

CONTEXT: {context}
QUERY: {query}"""
    
    # 3. Generate
    response = await llm_client.generate(prompt, max_tokens=settings.max_tokens)
    return response
```

## Testing

### Unit Tests
```bash
pytest tests/unit -v
pytest tests/unit/test_rag.py::test_retrieve_context -v
pytest --cov=app --cov-report=html
```

### Integration Test Example
```python
# tests/integration/test_rag.py
@pytest.mark.asyncio
async def test_rag_pipeline(test_client, sample_client_id):
    # Upload knowledge
    await upload_knowledge(sample_client_id, "Hours: 9 AM - 10 PM")
    
    # Query
    rag = RAGEngine()
    response = await rag.generate_response(
        "What time do you open?", sample_client_id
    )
    
    assert "9 AM" in response
```

## Commands Reference

```bash
# Development
uvicorn app.main:app --reload --port 8000
celery -A app.worker worker --loglevel=info

# Database
alembic revision --autogenerate -m "description"
alembic upgrade head
alembic downgrade -1

# Docker
docker-compose up -d
docker-compose logs -f api
docker-compose down

# Testing
pytest -v
pytest tests/unit -v --cov=app
pytest -m "not slow"

# Code quality
black app/ --line-length 100
ruff check app/ --fix
mypy app/
```

---
v1.0 | 2026-02-06